{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "dic_path='D:/RL_Finance/MLops/fslab/lab04'\n",
    "\n",
    "sys.path.append(dic_path)\n",
    "os.chdir(dic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\xiang\\_netrc\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "!wandb login 9cea57a44f7df81e3434dc972ef754421915e27e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxiangyexu\u001b[0m (\u001b[33mxiangyexu-university-of-waterloo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\RL_Finance\\MLops\\fslab\\lab04\\wandb\\run-20250223_134621-73yotprj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xiangyexu-university-of-waterloo/lightning_logs/runs/73yotprj' target=\"_blank\">clean-music-11</a></strong> to <a href='https://wandb.ai/xiangyexu-university-of-waterloo/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xiangyexu-university-of-waterloo/lightning_logs' target=\"_blank\">https://wandb.ai/xiangyexu-university-of-waterloo/lightning_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xiangyexu-university-of-waterloo/lightning_logs/runs/73yotprj' target=\"_blank\">https://wandb.ai/xiangyexu-university-of-waterloo/lightning_logs/runs/73yotprj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_JOB_TYPE=profile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:62: FutureWarning: Importing `CharErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `CharErrorRate` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "INFO: IAMParagraphs.setup(None): Loading IAM paragraph regions and lines...\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IAMParagraphs.setup(None): Loading IAM paragraph regions and lines...\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\lightning_fabric\\connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "INFO: Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\lightning\\pytorch\\profilers\\pytorch.py:537: `use_cuda` is deprecated, use `activities` argument instead\n",
      "INFO: You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO: IAMParagraphs.setup(fit): Loading IAM paragraph regions and lines...\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IAMParagraphs.setup(fit): Loading IAM paragraph regions and lines...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | ResnetTransformer  | 14.0 M | train\n",
      "1 | train_acc | MulticlassAccuracy | 0      | train\n",
      "2 | val_acc   | MulticlassAccuracy | 0      | train\n",
      "3 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "4 | val_cer   | CharacterErrorRate | 0      | train\n",
      "5 | test_cer  | CharacterErrorRate | 0      | train\n",
      "6 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "---------------------------------------------------------\n",
      "14.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.0 M    Total params\n",
      "55.955    Total estimated model params size (MB)\n",
      "137       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0\n",
      "cuda:0 cuda:0\n",
      "cuda:0\n",
      "cuda:0 cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "INFO: FIT Profiler Report\n",
      "Profile stats for: records\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*        63.19%       13.400s        95.33%       20.218s        5.054s       13.374s        62.36%       20.218s        5.055s             4  \n",
      "[pl][profile][Callback]ModelCheckpoint{'monitor': No...        20.73%        4.396s        21.54%        4.567s        4.567s        4.399s        20.52%        4.567s        4.567s             1  \n",
      "[pl][profile][Callback]ImageToTextTableLogger.on_val...         4.10%     868.818ms         4.85%        1.028s        1.028s     864.597ms         4.03%        1.028s        1.028s             1  \n",
      "                        [pl][profile]run_training_batch         0.00%     915.900us         3.18%     673.465ms     336.733ms     269.000us         0.00%     671.033ms     335.517ms             2  \n",
      "[pl][profile][LightningModule]TransformerLitModel.op...         2.20%     465.911ms         3.17%     672.549ms     336.275ms     466.390ms         2.17%     670.764ms     335.382ms             2  \n",
      "                                            aten::copy_         3.16%     670.313ms         3.37%     714.544ms     341.234us     632.895ms         2.95%     634.014ms     302.777us          2094  \n",
      "[pl][profile][Strategy]SingleDeviceStrategy.backward...         1.80%     382.421ms         1.81%     383.818ms     127.939ms     583.518ms         2.72%     584.667ms     194.889ms             3  \n",
      "                                               aten::to         0.12%      26.056ms         2.90%     615.919ms     445.995us      25.575ms         0.12%     528.310ms     382.556us          1381  \n",
      "                                         aten::_to_copy         0.28%      60.261ms         2.78%     589.863ms     483.098us      36.916ms         0.17%     502.735ms     411.740us          1221  \n",
      "[pl][profile][Strategy]SingleDeviceStrategy.training...         0.04%       9.473ms         1.46%     309.292ms     103.097ms       4.214ms         0.02%     306.110ms     102.037ms             3  \n",
      "autograd::engine::evaluate_function: ConvolutionBack...         0.01%       3.019ms         0.09%      19.486ms     309.297us     711.000us         0.00%     185.350ms       2.942ms            63  \n",
      "                                   ConvolutionBackward0         0.01%       1.981ms         0.08%      15.981ms     253.659us     394.000us         0.00%     177.388ms       2.816ms            63  \n",
      "                             aten::convolution_backward         0.06%      13.000ms         0.07%      13.999ms     222.208us     174.385ms         0.81%     176.994ms       2.809ms            63  \n",
      "                                           aten::conv2d         0.02%       4.610ms         0.15%      32.648ms     259.110us       1.102ms         0.01%     168.089ms       1.334ms           126  \n",
      "[pl][module]torch.nn.modules.transformer.Transformer...         0.01%       1.402ms         0.68%     144.762ms      48.254ms       1.087ms         0.01%     145.464ms      48.488ms             3  \n",
      "[pl][module]torch.nn.modules.container.Sequential: m...         0.01%       1.461ms         0.39%      82.653ms      27.551ms      51.000us         0.00%     141.409ms      47.136ms             3  \n",
      "                               Optimizer.step#Adam.step         0.13%      27.222ms         0.53%     111.796ms      37.265ms      14.854ms         0.07%     113.278ms      37.759ms             3  \n",
      "                                      aten::convolution         0.01%       1.787ms         0.05%       9.662ms     153.370us     392.000us         0.00%      79.998ms       1.270ms            63  \n",
      "                                     aten::_convolution         0.01%       1.946ms         0.04%       7.875ms     124.998us     397.000us         0.00%      79.606ms       1.264ms            63  \n",
      "autograd::engine::evaluate_function: ScaledDotProduc...         0.01%       1.082ms         0.10%      21.755ms     906.446us     380.000us         0.00%      79.136ms       3.297ms            24  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 21.208s\n",
      "Self CUDA time total: 21.444s\n",
      "\n",
      "INFO:lightning.pytorch.profilers.profiler:FIT Profiler Report\n",
      "Profile stats for: records\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*        63.19%       13.400s        95.33%       20.218s        5.054s       13.374s        62.36%       20.218s        5.055s             4  \n",
      "[pl][profile][Callback]ModelCheckpoint{'monitor': No...        20.73%        4.396s        21.54%        4.567s        4.567s        4.399s        20.52%        4.567s        4.567s             1  \n",
      "[pl][profile][Callback]ImageToTextTableLogger.on_val...         4.10%     868.818ms         4.85%        1.028s        1.028s     864.597ms         4.03%        1.028s        1.028s             1  \n",
      "                        [pl][profile]run_training_batch         0.00%     915.900us         3.18%     673.465ms     336.733ms     269.000us         0.00%     671.033ms     335.517ms             2  \n",
      "[pl][profile][LightningModule]TransformerLitModel.op...         2.20%     465.911ms         3.17%     672.549ms     336.275ms     466.390ms         2.17%     670.764ms     335.382ms             2  \n",
      "                                            aten::copy_         3.16%     670.313ms         3.37%     714.544ms     341.234us     632.895ms         2.95%     634.014ms     302.777us          2094  \n",
      "[pl][profile][Strategy]SingleDeviceStrategy.backward...         1.80%     382.421ms         1.81%     383.818ms     127.939ms     583.518ms         2.72%     584.667ms     194.889ms             3  \n",
      "                                               aten::to         0.12%      26.056ms         2.90%     615.919ms     445.995us      25.575ms         0.12%     528.310ms     382.556us          1381  \n",
      "                                         aten::_to_copy         0.28%      60.261ms         2.78%     589.863ms     483.098us      36.916ms         0.17%     502.735ms     411.740us          1221  \n",
      "[pl][profile][Strategy]SingleDeviceStrategy.training...         0.04%       9.473ms         1.46%     309.292ms     103.097ms       4.214ms         0.02%     306.110ms     102.037ms             3  \n",
      "autograd::engine::evaluate_function: ConvolutionBack...         0.01%       3.019ms         0.09%      19.486ms     309.297us     711.000us         0.00%     185.350ms       2.942ms            63  \n",
      "                                   ConvolutionBackward0         0.01%       1.981ms         0.08%      15.981ms     253.659us     394.000us         0.00%     177.388ms       2.816ms            63  \n",
      "                             aten::convolution_backward         0.06%      13.000ms         0.07%      13.999ms     222.208us     174.385ms         0.81%     176.994ms       2.809ms            63  \n",
      "                                           aten::conv2d         0.02%       4.610ms         0.15%      32.648ms     259.110us       1.102ms         0.01%     168.089ms       1.334ms           126  \n",
      "[pl][module]torch.nn.modules.transformer.Transformer...         0.01%       1.402ms         0.68%     144.762ms      48.254ms       1.087ms         0.01%     145.464ms      48.488ms             3  \n",
      "[pl][module]torch.nn.modules.container.Sequential: m...         0.01%       1.461ms         0.39%      82.653ms      27.551ms      51.000us         0.00%     141.409ms      47.136ms             3  \n",
      "                               Optimizer.step#Adam.step         0.13%      27.222ms         0.53%     111.796ms      37.265ms      14.854ms         0.07%     113.278ms      37.759ms             3  \n",
      "                                      aten::convolution         0.01%       1.787ms         0.05%       9.662ms     153.370us     392.000us         0.00%      79.998ms       1.270ms            63  \n",
      "                                     aten::_convolution         0.01%       1.946ms         0.04%       7.875ms     124.998us     397.000us         0.00%      79.606ms       1.264ms            63  \n",
      "autograd::engine::evaluate_function: ScaledDotProduc...         0.01%       1.082ms         0.10%      21.755ms     906.446us     380.000us         0.00%      79.136ms       3.297ms            24  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 21.208s\n",
      "Self CUDA time total: 21.444s\n",
      "\n",
      "c:\\Users\\xiang\\anaconda3\\lib\\site-packages\\lightning\\pytorch\\profilers\\pytorch.py:537: `use_cuda` is deprecated, use `activities` argument instead\n",
      "INFO: IAMParagraphs.setup(test): Loading IAM paragraph regions and lines...\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IAMParagraphs.setup(test): Loading IAM paragraph regions and lines...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/cer          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.7388653755187988     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     3.179500102996826     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/cer         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.7388653755187988    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    3.179500102996826    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"lightning_logs\", job_type=\"profile\")\n",
    "\n",
    "# make it easier to separate these from training runs\n",
    "%env WANDB_JOB_TYPE=profile\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = min(os.cpu_count(),8)  # change this number later and see how the results change\n",
    "gpus = 1  # must be run with accelerator\n",
    "\n",
    "\n",
    "%run training/personal_run_experiment.py --wandb --profile \\\n",
    "  --max_epochs=1 \\\n",
    "  --model_class=ResnetTransformer --data_class=PreloadedIAMParagraphs --loss=transformer \\\n",
    "  --batch_size={batch_size} --num_workers={num_workers} --precision=16 --gpus=1\n",
    "\n",
    "\n",
    "\n",
    "latest_expt = wandb.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>test/cer</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁█</td></tr><tr><td>validation/cer</td><td>▁</td></tr><tr><td>validation/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>test/cer</td><td>1.73887</td></tr><tr><td>test/loss</td><td>3.1795</td></tr><tr><td>trainer/global_step</td><td>6</td></tr><tr><td>validation/cer</td><td>1.78936</td></tr><tr><td>validation/loss</td><td>3.29187</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-music-11</strong> at: <a href='https://wandb.ai/xiangyexu-university-of-waterloo/lightning_logs/runs/73yotprj' target=\"_blank\">https://wandb.ai/xiangyexu-university-of-waterloo/lightning_logs/runs/73yotprj</a><br> View project at: <a href='https://wandb.ai/xiangyexu-university-of-waterloo/lightning_logs' target=\"_blank\">https://wandb.ai/xiangyexu-university-of-waterloo/lightning_logs</a><br>Synced 5 W&B file(s), 3 media file(s), 24 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250223_134621-73yotprj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#add execution trace to logged and versioned binaries\n",
    "folder = wandb.run.dir\n",
    "trace_matcher = wandb.run.dir + \"/*.pt.trace.json\"\n",
    "trace_file = glob.glob(trace_matcher)[0]\n",
    "trace_at = wandb.Artifact(name=f\"trace-{wandb.run.id}\", type=\"trace\")\n",
    "trace_at.add_file(trace_file, name=\"training_step.pt.trace.json\")\n",
    "wandb.log_artifact(trace_at)\n",
    "\n",
    "    \n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wandb.ai/xiangyexu-university-of-waterloo/lightning_logs/runs/73yotprj/tensorboard\n"
     ]
    }
   ],
   "source": [
    "your_tensorboard_url = latest_expt.url + \"/tensorboard\"\n",
    "\n",
    "print(your_tensorboard_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wandb.ai/xiangyexu-university-of-waterloo/lightning_logs/artifacts/trace/trace-73yotprj/latest/files/training_step.pt.trace.json\n"
     ]
    }
   ],
   "source": [
    "trace_files_url = latest_expt.url.split(\"/runs/\")[0] + f\"/artifacts/trace/trace-{latest_expt.id}/latest/files/\"\n",
    "trace_url = trace_files_url + \"training_step.pt.trace.json\"\n",
    "\n",
    "print(trace_url)\n",
    "## IFrame(src=trace_url, height=frame_height * 1.5, width=\"100%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
